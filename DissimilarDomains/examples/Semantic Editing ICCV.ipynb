{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f549effb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:33.865029Z",
     "start_time": "2023-03-24T04:03:33.750775Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./../')\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib_inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e9d43",
   "metadata": {},
   "source": [
    "For the sake of simplicity, most of the additional functions were implemented in the `nb_utils` module.\n",
    "\n",
    "See `General Results.ipynb` for information about basic operations with a trained model (loading, metrics evaluation, inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668306c",
   "metadata": {},
   "source": [
    "# Load checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af985ac",
   "metadata": {},
   "source": [
    "The semantic modifications we use in our work (`StyleSpace` and `StyleFlow`) produce results only for models with a resolution of $1024 \\times 1024$. So, here we will use those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2928b33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:01:03.200348Z",
     "start_time": "2023-03-24T04:01:02.968290Z"
    }
   },
   "outputs": [],
   "source": [
    "import nb_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e45e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:01:03.889057Z",
     "start_time": "2023-03-24T04:01:03.813225Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "base_exp_path = '~/StyleDomain/DissimilarDomains/training-runs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd32cc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:01:12.533801Z",
     "start_time": "2023-03-24T04:01:08.255590Z"
    }
   },
   "outputs": [],
   "source": [
    "exp_path = os.path.join(base_exp_path, '00069-Mega-stylegan2-kimg241-resumeffhq1024')\n",
    "_, G_ema, _, G_ema_base, options, metrics = nb_utils.load_checkpoint(\n",
    "    exp_path=exp_path, chkpt_idx=40, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ad18d",
   "metadata": {},
   "source": [
    "# StyleSpace Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c74582",
   "metadata": {},
   "source": [
    "First, let's define the list of available modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c30ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:01:12.688785Z",
     "start_time": "2023-03-24T04:01:12.588932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Source: https://github.com/betterze/StyleSpace/blob/main/StyleSpace_single.ipynb\n",
    "configs_ffhq = {\n",
    "    'Black Hair'        : [(12, 479)],\n",
    "    'Blond Hair'        : [(12, 479), (12, 266)],\n",
    "    'Grey Hair'         : [(11, 286)],\n",
    "    'Short Hair'        : [(6, 500), (8, 128), (5, 92), (6, 394), (6, 323)],\n",
    "    'Wavy Hair'         : [(6, 500), (8, 128), (5, 92), (6, 394), (6, 323)],\n",
    "    'Bangs'             : [(3, 259), (6, 285), (5, 414), (6, 128), (9, 295), (6, 322), (6, 487), (6, 504)],\n",
    "    'Receding Hairline' : [(5, 414), (6, 322), (6, 497), (6, 504)],\n",
    "    'Smile'             : [(6, 501)],\n",
    "    'Lipstick'          : [(15, 45)],\n",
    "    'Sideburns'         : [(12, 237)],\n",
    "    'Goatee'            : [(9, 421)],\n",
    "    'Earrings'          : [(8, 81)],\n",
    "    'Glasses'           : [(3, 288), (2, 175), (3, 120), (2, 97)],\n",
    "    'Wear Suit'         : [(9, 441), (8, 292), (11, 358), (6, 223)],\n",
    "    'Gender'            : [(9, 6)],\n",
    "    'Bangs'             : [(3, 169)],\n",
    "    'Gaze'              : [(9, 409)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11d4cd",
   "metadata": {},
   "source": [
    "`Generator` module was modified in order to apply single-channel modifications during inference. The following interface is used: one can pass several new named arguments into `Generator.synthesis` call:\n",
    "\n",
    "* `style_space_modifications` — List of modifications. Defaults to None.\n",
    "* `style_space_modifications_first` — Whether to apply Style Space modification before Style Space offsets. Defaults to False.\n",
    "* `saved_styles` — Buffer to save intermediate Style Space latents. Defaults to None.\n",
    "\n",
    "The final one is used to save activation statistics during the forward pass and then rescale modification magnitude. This is necessary to correctly apply multichannel modifications (i.e., **Wavy Hair**).\n",
    "\n",
    "`style_space_modifications` should be represented as a list of single-channel modifications. Each modification consists of a layer idx, a channel idx, a modification magnitude, and an additional value to suppress StyleDomain directions in the corresponding channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ed8ca",
   "metadata": {},
   "source": [
    "Let's define an additional function that will prepare `style_space_modifications` in the required format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8dc36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:01:15.208169Z",
     "start_time": "2023-03-24T04:01:15.113080Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_modifier(name, magnitude, offset_factor, styles_stats=None):\n",
    "    \"\"\"\n",
    "        :param str name: Name of modification for the FFHQ model\n",
    "        :param float magnitude: Modification magnitude\n",
    "        :param float offset_factor: Scalar multiplier in corresponding channels for StyleDomain directions\n",
    "        :param Optional[List[torch.Tensor]] styles_stats: Standard deviation of the StyleSpace will \n",
    "            be stored for each SyntesisNetwork layer.\n",
    "            \n",
    "        :return List[Tuple[Tuple[int, int], float, float]]: List of modifications \n",
    "            in the format of ((layer_idx, channel_idx), magnitude, offset_factor)\n",
    "    \"\"\"\n",
    "    modifier = []\n",
    "    for layer, channel in configs_ffhq[name]:\n",
    "        channel_std = styles_stats[layer][channel] if styles_stats is not None else 1.0\n",
    "        modifier.append(\n",
    "            ((layer, channel), magnitude * channel_std, offset_factor)\n",
    "        )\n",
    "    \n",
    "    return modifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415cd05",
   "metadata": {},
   "source": [
    "Take a modifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd5ebb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:01:15.899185Z",
     "start_time": "2023-03-24T04:01:15.801767Z"
    }
   },
   "outputs": [],
   "source": [
    "modifier = get_modifier('Smile', -8.0, 0.0)\n",
    "modifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031103f0",
   "metadata": {},
   "source": [
    "Generate images from fine-tuned model that was loaded previously (**Full** parameterization for **Mega** domain):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cade86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:01:20.002208Z",
     "start_time": "2023-03-24T04:01:17.411660Z"
    }
   },
   "outputs": [],
   "source": [
    "images, ws = nb_utils.generate_images(\n",
    "    G_ema, grid_size=6, device=device, truncation_psi=0.9, seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af06d53",
   "metadata": {},
   "source": [
    "Use the same latents in order to modify images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790016a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:01:21.610867Z",
     "start_time": "2023-03-24T04:01:21.200414Z"
    }
   },
   "outputs": [],
   "source": [
    "modified_images, _ = nb_utils.generate_images(\n",
    "    G_ema, grid_size=ws.shape[0], device=device, truncation_psi=0.9,\n",
    "    target_ws=torch.from_numpy(ws).to(device), style_space_modifications=modifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d158a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:01:26.102927Z",
     "start_time": "2023-03-24T04:01:22.999729Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_size = np.array([2, ws.shape[0]])\n",
    "fig, axes = plt.subplots(*grid_size, figsize=grid_size[::-1] * 2)\n",
    "axes = axes.reshape(grid_size)\n",
    "nb_utils.prepare_axes(axes)\n",
    "\n",
    "for ax, image in zip(axes[0].reshape(-1), images):\n",
    "    ax.imshow(image)\n",
    "for ax, image in zip(axes[1].reshape(-1), modified_images):\n",
    "    ax.imshow(image)\n",
    "\n",
    "axes[1, 0].set_ylabel('+ Smile', fontdict=dict(fontsize=12, weight='bold'))\n",
    "fig.suptitle('StyleSpace modifications for Mega')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679a87c",
   "metadata": {},
   "source": [
    "## Multichannel modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a72f1a",
   "metadata": {},
   "source": [
    "To apply modifications that change multiple channels at once, we have to compute the scale of each channel. See `training.networks.w_to_s` for an exact description of the `saved_styles` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325370e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:01:34.696171Z",
     "start_time": "2023-03-24T04:01:34.593403Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_styles_stats(G, n_batches=128, batch_size=8, **kwargs):\n",
    "    \"\"\"\n",
    "        :param torch.Module G: Generator\n",
    "        :param int n_batches: Number of batches to estimate variance\n",
    "        :param int batch_size: Batch size for `nb_utils.generate_images`\n",
    "        :param dict kwargs: Additional named parameters that will be passed to `nb_utils.generate_images`\n",
    "            I.e. device, truncation_psi, etc.\n",
    "        :return Dict[torch.Tensor]: Standard deviation for each layer and channel\n",
    "    \"\"\"\n",
    "    styles_stats = dict()\n",
    "    saved_styles = defaultdict(list)\n",
    "    \n",
    "    for idx in range(n_batches):\n",
    "        batch_styles = dict(initial=dict(), final=dict())\n",
    "        _ = nb_utils.generate_images(\n",
    "            G, grid_size=batch_size, batch_size=batch_size,\n",
    "            seed=idx, saved_styles=batch_styles, **kwargs\n",
    "        )\n",
    "        \n",
    "        for layer, styles in batch_styles['initial'].items():\n",
    "            saved_styles[layer].append(styles.numpy())\n",
    "    \n",
    "    for layer, styles in saved_styles.items():\n",
    "        styles_stats[layer] = np.std(np.vstack(styles), axis=0)\n",
    "        \n",
    "    return styles_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c5a3b",
   "metadata": {},
   "source": [
    "Compute statistics for the model using $128 \\times 8 = 1024$ images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d39c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:02:27.714310Z",
     "start_time": "2023-03-24T04:01:36.264075Z"
    }
   },
   "outputs": [],
   "source": [
    "styles_stats = compute_styles_stats(G_ema, n_batches=128, batch_size=8, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ad965",
   "metadata": {},
   "source": [
    "Apply a multichannel modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d4db9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:00.667103Z",
     "start_time": "2023-03-24T04:03:00.554970Z"
    }
   },
   "outputs": [],
   "source": [
    "modifier = get_modifier('Wavy Hair', 10.0, 0.0, styles_stats=styles_stats)\n",
    "modifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b074fc3",
   "metadata": {},
   "source": [
    "Use the same model as before to generate modified images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71f0d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:09.497969Z",
     "start_time": "2023-03-24T04:03:05.855064Z"
    }
   },
   "outputs": [],
   "source": [
    "images, ws = nb_utils.generate_images(\n",
    "    G_ema, grid_size=6, device=device, truncation_psi=0.9, seed=19\n",
    ")\n",
    "\n",
    "modified_images, _ = nb_utils.generate_images(\n",
    "    G_ema, grid_size=ws.shape[0], device=device, truncation_psi=0.9,\n",
    "    target_ws=torch.from_numpy(ws).to(device), style_space_modifications=modifier\n",
    ")\n",
    "\n",
    "grid_size = np.array([2, ws.shape[0]])\n",
    "fig, axes = plt.subplots(*grid_size, figsize=grid_size[::-1] * 2)\n",
    "axes = axes.reshape(grid_size)\n",
    "nb_utils.prepare_axes(axes)\n",
    "\n",
    "for ax, image in zip(axes[0].reshape(-1), images):\n",
    "    ax.imshow(image)\n",
    "for ax, image in zip(axes[1].reshape(-1), modified_images):\n",
    "    ax.imshow(image)\n",
    "\n",
    "axes[1, 0].set_ylabel('+ Wavy Hair', fontdict=dict(fontsize=12, weight='bold'))\n",
    "fig.suptitle('StyleSpace modifications for Mega')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59274bae",
   "metadata": {},
   "source": [
    "# StyleFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2132b2",
   "metadata": {},
   "source": [
    "We use the original StyleFlow implementation from [https://github.com/RameenAbdal/StyleFlow](https://github.com/RameenAbdal/StyleFlow). We created a simple wrapper to apply modifications directly in a Jupyter notebook. \n",
    "\n",
    "Note that this method requires initial values for attributes that are going to be modified. Those values could be obtained via the [Microsoft Face API](https://azure.microsoft.com/en-us/products/cognitive-services/face/) and [DPR model](https://github.com/zhhoper/DPR). Instead, we chose to use images that were labeled by StyleFlow authors, as they provide $1000$ labeled latent vectors in $W+$ space that came from the same model that we use as an initial checkpoint to finetune our models. \n",
    "\n",
    "Since **Affine+**, **AffineLight+**, and **StyleSpace** parameterizations do not update the `Mapping Network` during training, those vectors are valid $W+$ space latents for finetuned models as well. However, in **Full** parameterization the `Mapping Network` changes, so we should perform image inversion into $Z$ space to obtain the correct $W+$ latents. Instead, we chose to ignore changes in the `Mapping Network` and use the $W+$ vectors as they are. We observe that this does not influence modifications' quality due to model alignment (see [StyleAlign](https://openreview.net/pdf?id=Qg2vi4ZbHM9))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a28b9a-ce6a-44d3-b507-34067c5917b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# styleflow weights\n",
    "\n",
    "!wget https://nxt.2a2i.org/index.php/s/yxdCXxSWJgAKXkP/download/styleflow_data.zip -O ../editing/styleflow/styleflow_data.zip\n",
    "!unzip ../editing/styleflow/styleflow_data.zip -d ../editing/styleflow\n",
    "!rm -rf ../editing/styleflow/styleflow_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f78ce83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:14.746141Z",
     "start_time": "2023-03-24T04:03:14.642747Z"
    }
   },
   "outputs": [],
   "source": [
    "styleflow_data_path = './../editing/styleflow/styleflow_data/data'\n",
    "styleflow_model_path = './../editing/styleflow/styleflow_data/flow_weight/modellarge10k.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8312e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:16.165133Z",
     "start_time": "2023-03-24T04:03:15.880726Z"
    }
   },
   "outputs": [],
   "source": [
    "from editing.styleflow.editor import StyleFlowEditor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81836b9",
   "metadata": {},
   "source": [
    "Create a wrapper for StyleFlow semantic modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d166ef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:18.918610Z",
     "start_time": "2023-03-24T04:03:18.724870Z"
    }
   },
   "outputs": [],
   "source": [
    "editor = StyleFlowEditor(styleflow_data_path, styleflow_model_path, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c5ffb",
   "metadata": {},
   "source": [
    "Initialize the model to modify one of the labeled images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33692235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:21.772092Z",
     "start_time": "2023-03-24T04:03:21.672814Z"
    }
   },
   "outputs": [],
   "source": [
    "editor._allocate_entity(idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372f682b",
   "metadata": {},
   "source": [
    "We can change any of those attributes:\n",
    "```python\n",
    "attr_order = ['Gender', 'Glasses', 'Yaw', 'Pitch', 'Baldness', 'Beard', 'Age', 'Expression']\n",
    "```\n",
    "\n",
    "Change `Gender` using StyleFlow. `edit_power` changes between $0.0$ and $1.0$, where $0.0$ corresponds to the minimum attribute value and $1.0$ corresponds to the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf430cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:23.326808Z",
     "start_time": "2023-03-24T04:03:23.022645Z"
    }
   },
   "outputs": [],
   "source": [
    "ws, ws_modified = editor.real_time_editing(attr_index=0, edit_power=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc264e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T07:11:49.332319Z",
     "start_time": "2023-01-19T07:11:49.228265Z"
    }
   },
   "source": [
    "Generate images from the latents using **Full** parameterization model for the **Mega** domain and show them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fd019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:38.785270Z",
     "start_time": "2023-03-24T04:03:37.954275Z"
    }
   },
   "outputs": [],
   "source": [
    "images, _ = nb_utils.generate_images(\n",
    "    G_ema, grid_size=ws.shape[0], device=device, truncation_psi=0.9,\n",
    "    target_ws=ws\n",
    ")\n",
    "modified_images, _ = nb_utils.generate_images(\n",
    "    G_ema, grid_size=ws_modified.shape[0], device=device, truncation_psi=0.9,\n",
    "    target_ws=ws_modified\n",
    ")\n",
    "\n",
    "grid_size = np.array([2, ws.shape[0]])\n",
    "fig, axes = plt.subplots(*grid_size, figsize=grid_size[::-1] * 2)\n",
    "axes = axes.reshape(grid_size)\n",
    "nb_utils.prepare_axes(axes)\n",
    "\n",
    "for ax, image in zip(axes[0].reshape(-1), images):\n",
    "    ax.imshow(image)\n",
    "for ax, image in zip(axes[1].reshape(-1), modified_images):\n",
    "    ax.imshow(image)\n",
    "\n",
    "axes[1, 0].set_ylabel('Gender', fontdict=dict(fontsize=12, weight='bold'))\n",
    "fig.suptitle('StyleFlow modifications for Mega')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1714f8",
   "metadata": {},
   "source": [
    "Apply `Yaw` modification to multiple images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875888d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:41.985552Z",
     "start_time": "2023-03-24T04:03:40.744280Z"
    }
   },
   "outputs": [],
   "source": [
    "ws = []\n",
    "ws_modified = []\n",
    "for image_idx in range(6):\n",
    "    editor._allocate_entity(image_idx)\n",
    "    w, w_modified = editor.real_time_editing(attr_index=2, edit_power=0.9)\n",
    "    ws.append(w)\n",
    "    ws_modified.append(w_modified)\n",
    "ws = torch.vstack(ws)\n",
    "ws_modified = torch.vstack(ws_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759bf57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:46.741010Z",
     "start_time": "2023-03-24T04:03:43.307820Z"
    }
   },
   "outputs": [],
   "source": [
    "images, _ = nb_utils.generate_images(\n",
    "    G_ema, grid_size=1, device=device, truncation_psi=0.9,\n",
    "    target_ws=ws\n",
    ")\n",
    "modified_images, _ = nb_utils.generate_images(\n",
    "    G_ema, grid_size=1, device=device, truncation_psi=0.9,\n",
    "    target_ws=ws_modified\n",
    ")\n",
    "\n",
    "grid_size = np.array([2, ws.shape[0]])\n",
    "fig, axes = plt.subplots(*grid_size, figsize=grid_size[::-1] * 2)\n",
    "axes = axes.reshape(grid_size)\n",
    "nb_utils.prepare_axes(axes)\n",
    "\n",
    "for ax, image in zip(axes[0].reshape(-1), images):\n",
    "    ax.imshow(image)\n",
    "for ax, image in zip(axes[1].reshape(-1), modified_images):\n",
    "    ax.imshow(image)\n",
    "\n",
    "axes[1, 0].set_ylabel('Yaw', fontdict=dict(fontsize=12, weight='bold'))\n",
    "fig.suptitle('StyleFlow modifications for Mega')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0805c",
   "metadata": {},
   "source": [
    "# Results reproduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268cf19",
   "metadata": {},
   "source": [
    "To reproduce all results, all necessary models must be trained (in $1024\\times 1024$ resolution). More precisely, it is the cartesian product of datasets and parameterizations ($20$ models in total):\n",
    "\n",
    "$$[\\text{Metfaces}, \\text{Mega}, \\text{Ukiyoe}, \\text{Dog}, \\text{Cat}] \\times [\\text{Full}, \\text{Affine}+, \\text{AffineLight}+, \\text{StyleSpace}]$$\n",
    "\n",
    "Let's define a list of all those experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c80aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:03:54.977302Z",
     "start_time": "2023-03-24T04:03:54.867729Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "metfaces = {\n",
    "    'Full':         '00103-metfaces-stylegan2-kimg241-resumeffhq1024',\n",
    "    'Affine+':      '00302-metfaces-stylegan2-glrate0.02-kimg241-resumeffhq1024-Gparts-synt_affine,tRGB_affine,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive',\n",
    "    'StyleSpace':   '00227-metfaces-stylegan2-glrate0.02-kimg241-resumeffhq1024-Gparts-synt_offset,tRGB_offset-use-dom-mod-additive',\n",
    "    'AffineLight+': '00325-metfaces-stylegan2-glrate0.008-kimg241-resumeffhq1024-Gparts-synt_affine_weights_offset,tRGB_affine_weights_offset,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive,affine_out_in_5_1_additive',\n",
    "}\n",
    "Mega = {\n",
    "    'Full':         '00069-Mega-stylegan2-kimg241-resumeffhq1024',\n",
    "    'Affine+':      '00301-Mega-stylegan2-glrate0.02-kimg241-resumeffhq1024-Gparts-synt_affine,tRGB_affine,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive',\n",
    "    'StyleSpace':   '00228-Mega-stylegan2-glrate0.02-kimg241-resumeffhq1024-Gparts-synt_offset,tRGB_offset-use-dom-mod-additive',\n",
    "    'AffineLight+': '00325-Mega-stylegan2-glrate0.008-kimg241-resumeffhq1024-Gparts-synt_affine_weights_offset,tRGB_affine_weights_offset,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive,affine_out_in_5_1_additive',\n",
    "}\n",
    "ukiyoe = {\n",
    "    'Full':         '00106-ukiyoe-stylegan2-kimg241-resumeffhq1024',\n",
    "    'Affine+':      '00302-ukiyoe-stylegan2-glrate0.02-kimg241-resumeffhq1024-Gparts-synt_affine,tRGB_affine,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive',\n",
    "    'StyleSpace':   '00227-ukiyoe-stylegan2-glrate0.02-kimg241-resumeffhq1024-Gparts-synt_offset,tRGB_offset-use-dom-mod-additive',\n",
    "    'AffineLight+': '00319-ukiyoe-stylegan2-glrate0.008-kimg241-resumeffhq1024-Gparts-synt_affine_weights_offset,tRGB_affine_weights_offset,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive,affine_out_in_5_1_additive',\n",
    "}\n",
    "afhqdog_1024 = {\n",
    "    'Full':         '00219-afhqdog_1024-stylegan2-kimg241-resumeffhq1024',\n",
    "    'Affine+':      '00301-afhqdog_1024-stylegan2-glrate0.02-kimg241-resumeffhq1024-Gparts-synt_affine,tRGB_affine,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive',\n",
    "    'StyleSpace':   '00219-afhqdog_1024-stylegan2-glrate0.02-kimg241-resumeffhq1024-Gparts-synt_offset,tRGB_offset-use-dom-mod-additive',\n",
    "    'AffineLight+': '00325-afhqdog_1024-stylegan2-glrate0.008-kimg241-resumeffhq1024-Gparts-synt_affine_weights_offset,tRGB_affine_weights_offset,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive,affine_out_in_5_1_additive',\n",
    "}\n",
    "afhqcat_1024 = {\n",
    "    'Full':         '00218-afhqcat_1024-stylegan2-kimg241-resumeffhq1024',\n",
    "    'Affine+':      '00307-afhqcat_1024-stylegan2-glrate0.02-kimg241-resumeffhq1024-Gparts-synt_affine,tRGB_affine,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive',\n",
    "    'StyleSpace':   '00218-afhqcat_1024-stylegan2-glrate0.02-kimg241-resumeffhq1024-Gparts-synt_offset,tRGB_offset-use-dom-mod-additive',\n",
    "    'AffineLight+': '00326-afhqcat_1024-stylegan2-glrate0.008-kimg241-resumeffhq1024-Gparts-synt_affine_weights_offset,tRGB_affine_weights_offset,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive,affine_out_in_5_1_additive',\n",
    "}\n",
    "\n",
    "semantic_edditing_experiments = {\n",
    "    'Metfaces': metfaces,\n",
    "    'Mega': Mega,\n",
    "    'Ukiyoe': ukiyoe,\n",
    "    'Dog': afhqdog_1024,\n",
    "    'Cat': afhqcat_1024,\n",
    "}\n",
    "\n",
    "chkpt_idxs = {\n",
    "    'Metfaces': 180,\n",
    "    'Mega': 40,\n",
    "    'Ukiyoe': 100,\n",
    "    'Dog': 180,\n",
    "    'Cat': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f35354",
   "metadata": {},
   "source": [
    "Load all checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08d409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:04:53.687757Z",
     "start_time": "2023-03-24T04:03:58.315562Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_models = defaultdict(defaultdict)\n",
    "\n",
    "for dataset_name, dataset_exps in semantic_edditing_experiments.items():\n",
    "    chkpt_idx = chkpt_idxs[dataset_name]\n",
    "    \n",
    "    for parameterization_name, exp_suffix in dataset_exps.items():\n",
    "        exp_path = os.path.join(base_exp_path, exp_suffix)\n",
    "        _, G_ema, _, G_ema_base, _, metrics = nb_utils.load_checkpoint(\n",
    "            exp_path=exp_path, chkpt_idx=chkpt_idx, device=torch.device('cpu')\n",
    "        )\n",
    "        \n",
    "        all_models[parameterization_name][dataset_name] = G_ema\n",
    "    all_models['Original'][dataset_name] = G_ema_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54233d01",
   "metadata": {},
   "source": [
    "Let's define a function that will apply modifications for each model to a given image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def57ee6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:05:11.288597Z",
     "start_time": "2023-03-24T04:05:11.162801Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_modifications_figure(models, modifications, dataset_name, editor, image_idx, device):\n",
    "    grid_size = np.array([len(models), len(modifications)])\n",
    "    fig, axes = plt.subplots(*grid_size, figsize=grid_size[::-1] * 2)\n",
    "    axes = axes.reshape(grid_size)\n",
    "    nb_utils.prepare_axes(axes)\n",
    "\n",
    "    styles_stats = compute_styles_stats(models['Full'][dataset_name], n_batches=128, batch_size=8, device=device)\n",
    "    editor._allocate_entity(image_idx)\n",
    "    w = editor.initial_w\n",
    "\n",
    "    for idx, pname in enumerate(['Original', 'Full', 'Affine+', 'AffineLight+', 'StyleSpace']):\n",
    "        for jdx, (attribute, power) in enumerate(modifications):\n",
    "            if attribute is None:\n",
    "                # Do not apply modifications\n",
    "                title = ''\n",
    "\n",
    "                w_modified = w\n",
    "                modifier = None\n",
    "            elif isinstance(attribute, str):\n",
    "                # Apply StyleSpace modifications\n",
    "                title = attribute\n",
    "\n",
    "                w_modified = w\n",
    "                modifier = get_modifier(attribute, power, 0.0, styles_stats=styles_stats)\n",
    "            else:\n",
    "                # Apply StyleFlow modifications\n",
    "                title = editor.attr_order[attribute]\n",
    "                if attribute == 6:\n",
    "                    title = 'Rejuvenation' if power < 0.5 else 'Aging'\n",
    "\n",
    "                _, w_modified = editor.real_time_editing(attr_index=attribute, edit_power=power)\n",
    "                modifier = None\n",
    "\n",
    "            modified_images, _ = nb_utils.generate_images(\n",
    "                models[pname][dataset_name], grid_size=w_modified.shape[0], device=device,\n",
    "                truncation_psi=0.9, target_ws=w_modified.to(device), style_space_modifications=modifier\n",
    "            )\n",
    "\n",
    "            axes[idx, jdx].imshow(modified_images[0])\n",
    "            if idx == 0:\n",
    "                axes[idx, jdx].set_title(title, fontdict=dict(fontsize=12, weight='bold'))\n",
    "            if jdx == 0:\n",
    "                axes[idx, jdx].set_ylabel(pname, fontdict=dict(fontsize=12, weight='bold'))\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d00d30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T08:17:46.578911Z",
     "start_time": "2023-01-19T08:17:46.441570Z"
    }
   },
   "source": [
    "### Figure 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5631a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:08:57.044485Z",
     "start_time": "2023-03-24T04:07:36.586092Z"
    }
   },
   "outputs": [],
   "source": [
    "image_idx, is_smile, is_yaw_left, is_man, is_gaze_left = 32, False, True, False, True\n",
    "\n",
    "mega_modifications = [\n",
    "    (None, None), \n",
    "    ('Blond Hair', -4.0), ('Black Hair', 4.0), \n",
    "    ('Smile', 3.0 if is_smile else -1.0), \n",
    "    ('Lipstick', -3.0),\n",
    "    (2, 0.9 if is_yaw_left else 0.1), \n",
    "    (6, 0.1), (6, 0.9), \n",
    "    (0, 0.3 if is_man else 0.7),\n",
    "    ('Short Hair', -4.0), ('Wavy Hair', 4.0),             \n",
    "    ('Gaze', 6.0 if is_gaze_left else -6.0),\n",
    "]\n",
    "\n",
    "fig = make_modifications_figure(\n",
    "    models=all_models, \n",
    "    modifications=mega_modifications, \n",
    "    dataset_name='Mega', editor=editor, image_idx=image_idx, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75644a67",
   "metadata": {},
   "source": [
    "### Figure 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3789f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:10:19.929947Z",
     "start_time": "2023-03-24T04:08:57.047657Z"
    }
   },
   "outputs": [],
   "source": [
    "image_idx, is_smile, is_yaw_left, is_man, is_gaze_left = 23, False, False, True, True\n",
    "\n",
    "metfaces_modifications = [\n",
    "    (None, None), \n",
    "    ('Blond Hair', -4.0), ('Black Hair', 4.0), \n",
    "    ('Smile', 3.0 if is_smile else -1.0), \n",
    "    ('Lipstick', -3.0),\n",
    "    (2, 0.9 if is_yaw_left else 0.1), \n",
    "    (6, 0.1), (6, 0.9), \n",
    "    (0, 0.3 if is_man else 0.7),\n",
    "    ('Short Hair', -4.0), ('Wavy Hair', 4.0),             \n",
    "    ('Gaze', 6.0 if is_gaze_left else -6.0),\n",
    "]\n",
    "\n",
    "fig = make_modifications_figure(\n",
    "    models=all_models, \n",
    "    modifications=metfaces_modifications, \n",
    "    dataset_name='Metfaces', editor=editor, image_idx=image_idx, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdef81c",
   "metadata": {},
   "source": [
    "### Figure 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b2732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:13:58.007852Z",
     "start_time": "2023-03-24T04:12:35.273984Z"
    }
   },
   "outputs": [],
   "source": [
    "image_idx, is_smile, is_yaw_left, is_man, is_gaze_left = 22, False, True, False, False\n",
    "\n",
    "ukiyoe_modifications = [\n",
    "    (None, None), \n",
    "    ('Blond Hair', -8.0), ('Black Hair', 8.0), \n",
    "    ('Smile', 4.0 if is_smile else -4.0), \n",
    "    ('Lipstick', -6.0),\n",
    "    (2, 0.6 if is_yaw_left else 0.4), \n",
    "    (6, 0.2), (6, 1.0), \n",
    "    (0, 0.0 if is_man else 1.0),\n",
    "    ('Short Hair', -5.0), ('Wavy Hair', 3.0),             \n",
    "    ('Gaze', 4.0 if is_gaze_left else -4.0),\n",
    "]\n",
    "\n",
    "fig = make_modifications_figure(\n",
    "    models=all_models, \n",
    "    modifications=ukiyoe_modifications, \n",
    "    dataset_name='Ukiyoe', editor=editor, image_idx=image_idx, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9889a5",
   "metadata": {},
   "source": [
    "### Figure 25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1f9c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:15:04.549361Z",
     "start_time": "2023-03-24T04:13:58.011629Z"
    }
   },
   "outputs": [],
   "source": [
    "image_idx, is_yaw_left, is_gaze_left = 10, False, False\n",
    "\n",
    "afhqcat_modifications = [\n",
    "    (None, None), \n",
    "    ('Blond Hair', -5.0), ('Black Hair', 5.0), \n",
    "    (2, 1.0 if is_yaw_left else 0.0), \n",
    "    ('Gaze', 6.0 if is_gaze_left else -6.0),\n",
    "    ('Short Hair', -6.0), ('Wavy Hair', 6.0), \n",
    "]\n",
    "\n",
    "fig = make_modifications_figure(\n",
    "    models=all_models, \n",
    "    modifications=afhqcat_modifications, \n",
    "    dataset_name='Cat', editor=editor, image_idx=image_idx, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7dcd3",
   "metadata": {},
   "source": [
    "### Figure 25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908a7d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T04:16:10.626287Z",
     "start_time": "2023-03-24T04:15:04.552674Z"
    }
   },
   "outputs": [],
   "source": [
    "image_idx, is_yaw_left, is_man = 38, True, False\n",
    "\n",
    "afhqdog_modifications = [\n",
    "    (None, None), \n",
    "    ('Blond Hair', -5.0), ('Black Hair', 4.0), \n",
    "    (2, 0.9 if is_yaw_left else 0.1), \n",
    "    (0, 0.0 if is_man else 1.0),\n",
    "    ('Short Hair', -4.0), ('Wavy Hair', 4.0), \n",
    "]\n",
    "\n",
    "fig = make_modifications_figure(\n",
    "    models=all_models, \n",
    "    modifications=afhqdog_modifications, \n",
    "    dataset_name='Dog', editor=editor, image_idx=image_idx, device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
