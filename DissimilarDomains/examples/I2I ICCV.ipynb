{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de96ff9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:04:19.129199Z",
     "start_time": "2023-03-15T18:04:10.636133Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./../')\n",
    "\n",
    "import glob\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib_inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf34e1",
   "metadata": {},
   "source": [
    "For the sake of simplicity, most of the additional functions were implemented in the `nb_utils` module.\n",
    "\n",
    "See `General Results.ipynb` for information about basic operations with a trained model (loading, metrics evaluation, inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee98668",
   "metadata": {},
   "source": [
    "# $Z$ space projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7510e5",
   "metadata": {},
   "source": [
    "We use the same approach to inverse images into latent space of `Generator` as the authors of the [StyleAlign](https://openreview.net/pdf?id=Qg2vi4ZbHM9) paper. We reimplemented [their approach](https://github.com/betterze/StyleAlign/blob/main/projector_z.py) on top of the original $W$ space inversion from the StyleGAN-ADA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4752c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:04:21.417708Z",
     "start_time": "2023-03-15T18:04:20.570195Z"
    }
   },
   "outputs": [],
   "source": [
    "import nb_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40c963",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:04:21.500210Z",
     "start_time": "2023-03-15T18:04:21.423128Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "base_exp_path = '~/StyleDomain/DissimilarDomains/training-runs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f6b78",
   "metadata": {},
   "source": [
    "Inversion can be performed using the `projector.py` script by specifying the path to a single image (or to the images folder). An inverse of a sample image created with a model fine-tuned to the **Cat** domain in **Full** parameterization is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e1802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:04:21.560708Z",
     "start_time": "2023-03-15T18:04:21.503327Z"
    }
   },
   "outputs": [],
   "source": [
    "image_path = './../samples/41_0.jpg'\n",
    "outdir_path = './inverted_samples/41_0/'\n",
    "network_path = os.path.join(base_exp_path, '00065-afhqcat-stylegan2-kimg241-resumeffhq512', 'network-snapshot-000241.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766811b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T01:47:07.057137Z",
     "start_time": "2023-03-15T01:43:37.655670Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "! python ./../projector.py --target \"$image_path\" --outdir \"$outdir_path\" --network \"$network_path\" \\\n",
    "    --space 'z' --truncation-psi 0.7  --num-steps 1000 \\\n",
    "    --gpu 0 --save-image --save-video --save-all-steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ebd2cb",
   "metadata": {},
   "source": [
    "As a result of this, the method generates the following files:\n",
    "\n",
    "* `x_target.png` — image that was projected to the latent space\n",
    "* `x_proj.png` — image obtained from the resulting latent \n",
    "* `x_proj.mp4` — amination of projection procedure\n",
    "\n",
    "\n",
    "* `x_projected_z.npz` — stores dictionary `{'z': torch.Tensor, 'z_steps': torch.Tensor}` — final and all intermediate latents during optimization for the image\n",
    "* `projected_z.npz` — stores dictionary `{'z': torch.Tensor, 'z_steps': torch.Tensor}` — final and all intermediate latents during optimization for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30ef43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:04:23.359929Z",
     "start_time": "2023-03-15T18:04:23.127831Z"
    }
   },
   "outputs": [],
   "source": [
    "! ls \"$outdir_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7893328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:04:23.562563Z",
     "start_time": "2023-03-15T18:04:23.365707Z"
    }
   },
   "outputs": [],
   "source": [
    "result = np.load(os.path.join(outdir_path, '0_projected_z.npz'), allow_pickle=True)\n",
    "result['z'].shape, result['z_steps'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd4d1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T12:58:10.925900Z",
     "start_time": "2023-01-20T12:58:10.577684Z"
    }
   },
   "source": [
    "| Original image                | Image after $Z$ space inversion                        |\n",
    "|-------------------------------|--------------------------------------------------------|\n",
    "| ![img](./../samples/41_0.jpg) | ![img](./inverted_samples/41_0/0_proj.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa9486",
   "metadata": {},
   "source": [
    "# Unconditional I2I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ad48d",
   "metadata": {},
   "source": [
    "Then we can use models from other domains to perform inference from the obtained latent vector. For example, here we use the model for the **Dog** domain in **Full** parameterization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e7ce9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:04:37.880905Z",
     "start_time": "2023-03-15T18:04:24.903912Z"
    }
   },
   "outputs": [],
   "source": [
    "exp_path = os.path.join(base_exp_path, '00066-afhqdog-stylegan2-kimg241-resumeffhq512')\n",
    "_, G_ema, _, _, _, _ = nb_utils.load_checkpoint(exp_path=exp_path, chkpt_idx=241, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a1660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T21:43:22.081649Z",
     "start_time": "2023-01-24T21:43:08.429999Z"
    }
   },
   "outputs": [],
   "source": [
    "images, _ = nb_utils.generate_images(\n",
    "    G_ema, grid_size=1, device=device, truncation_psi=0.8,\n",
    "    target_zs=torch.tensor(result['z']).to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f472cb29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T21:43:26.186615Z",
     "start_time": "2023-01-24T21:43:22.084905Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_size = np.array([1, 2])\n",
    "fig, axes = plt.subplots(*grid_size, figsize=grid_size[::-1] * 4)\n",
    "\n",
    "nb_utils.prepare_axes(axes)\n",
    "axes[0].imshow(plt.imread(image_path))\n",
    "axes[1].imshow(images[0])\n",
    "\n",
    "axes[0].set_title('Input', fontdict=dict(fontsize=12, weight='bold'))\n",
    "axes[1].set_title('Full', fontdict=dict(fontsize=12, weight='bold'))\n",
    "    \n",
    "fig.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15918c2c",
   "metadata": {},
   "source": [
    "# Reference-based I2I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc94d9",
   "metadata": {},
   "source": [
    "For reference-based I2I, we need to get the latent vector of the reference image. Here we will perform I2I using the same source **Cat** image and a **Dog** reference image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95022798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:04:37.930175Z",
     "start_time": "2023-03-15T18:04:37.883359Z"
    }
   },
   "outputs": [],
   "source": [
    "ref_image_path = './../samples/pixabay_dog_003552.jpg'\n",
    "ref_outdir_path = './inverted_samples/pixabay_dog_003552/'\n",
    "ref_network_path = os.path.join(base_exp_path, '00066-afhqdog-stylegan2-kimg241-resumeffhq512', 'network-snapshot-000241.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06919333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T21:43:30.728581Z",
     "start_time": "2023-01-24T21:43:30.631056Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! python ./../projector.py --target \"$ref_image_path\" --outdir \"$ref_outdir_path\" --network \"$ref_network_path\" \\\n",
    "    --space 'z' --truncation-psi 0.7  --num-steps 1000 \\\n",
    "    --gpu 0 --save-image --save-video --save-all-steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce50d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T21:43:33.545393Z",
     "start_time": "2023-01-24T21:43:33.415093Z"
    }
   },
   "outputs": [],
   "source": [
    "ref_result = np.load(os.path.join(ref_outdir_path, '0_projected_z.npz'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53356ad1",
   "metadata": {},
   "source": [
    "Then, we should combine latent vectors for source and reference images. We combine the first $6$ style codes from the source image with the latest codes from the reference image. \n",
    "\n",
    "Note that although transformation is defined in Style Space we can define it equivalently in $W+$ space. For example, the first $6$ style codes have one-to-one correspondence to the first $4$ latent vectors in $W+$ space (for StyleGAN2 in $512\\times 512$ resolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3bceef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T21:43:36.008732Z",
     "start_time": "2023-01-24T21:43:35.729472Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_slice = 4\n",
    "G_ema.to(device).eval()\n",
    "\n",
    "c = torch.empty([result['z'].shape[0], 0])\n",
    "# Generate a W+ latent for the source image\n",
    "ws = G_ema.mapping(\n",
    "    torch.tensor(result['z']).to(device), c.to(device), truncation_psi=0.8\n",
    ")\n",
    "# Generate a W+ latent for the reference image\n",
    "ref_ws = G_ema.mapping(\n",
    "    torch.tensor(ref_result['z']).to(device), c.to(device), truncation_psi=0.8\n",
    ")\n",
    "# Combine latent vectors (which is equivalent to combining style codes)\n",
    "ws[:, reference_slice:] = ref_ws[:, reference_slice:]\n",
    "\n",
    "# Generate an image from the latent vector\n",
    "images, _ = nb_utils.generate_images(\n",
    "    G_ema, grid_size=1, device=device,\n",
    "    target_ws=ws\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d8638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T21:43:37.918710Z",
     "start_time": "2023-01-24T21:43:37.161326Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_size = np.array([1, 3])\n",
    "fig, axes = plt.subplots(*grid_size, figsize=grid_size[::-1] * 4)\n",
    "\n",
    "nb_utils.prepare_axes(axes)\n",
    "axes[0].imshow(plt.imread(image_path))\n",
    "axes[1].imshow(plt.imread(ref_image_path))\n",
    "axes[2].imshow(images[0])\n",
    "\n",
    "axes[0].set_title('Source', fontdict=dict(fontsize=12, weight='bold'))\n",
    "axes[1].set_title('Reference', fontdict=dict(fontsize=12, weight='bold'))\n",
    "axes[2].set_title('Full', fontdict=dict(fontsize=12, weight='bold'))\n",
    "    \n",
    "fig.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e8281",
   "metadata": {},
   "source": [
    "# Results reproduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc7d82",
   "metadata": {},
   "source": [
    "To reproduce all results, all necessary models must be trained (in $512\\times 512$ resolution). More specifically, it is the following configurations ($9$ models in total):\n",
    "\n",
    "$$[\\text{Dog}, \\text{Cat}, \\text{Wild}] \\times [\\text{Full}, \\text{Affine}+, \\text{AffineLight}+]$$\n",
    "\n",
    "Let's define a list of all those experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad721ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:04:37.980498Z",
     "start_time": "2023-03-15T18:04:37.931959Z"
    }
   },
   "outputs": [],
   "source": [
    "afhqdog = {\n",
    "    'Full':         '00066-afhqdog-stylegan2-kimg241-resumeffhq512',\n",
    "    'Affine+':      '00313-afhqdog-stylegan2-glrate0.02-kimg241-resumeffhq512-Gparts-synt_affine,tRGB_affine,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive',\n",
    "    'AffineLight+': '00253-afhqdog-stylegan2-glrate0.008-kimg241-resumeffhq512-Gparts-synt_affine_weights_offset,tRGB_affine_weights_offset,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive,affine_out_in_5_1_additive',\n",
    "}\n",
    "afhqcat = {   \n",
    "    'Full':         '00065-afhqcat-stylegan2-kimg241-resumeffhq512',\n",
    "    'Affine+':      '00299-afhqcat-stylegan2-glrate0.02-kimg241-resumeffhq512-Gparts-synt_affine,tRGB_affine,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive',\n",
    "    'AffineLight+': '00253-afhqcat-stylegan2-glrate0.008-kimg241-resumeffhq512-Gparts-synt_affine_weights_offset,tRGB_affine_weights_offset,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive,affine_out_in_5_1_additive',\n",
    "}\n",
    "\n",
    "afhqwild = {   \n",
    "    'Full':         '00318-afhqwild-stylegan2-kimg241-resumeffhq512',\n",
    "    'Affine+':      '00319-afhqwild-stylegan2-glrate0.02-kimg241-resumeffhq512-Gparts-synt_affine,tRGB_affine,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive',\n",
    "    'AffineLight+': '00323-afhqwild-stylegan2-glrate0.008-kimg241-resumeffhq512-Gparts-synt_affine_weights_offset,tRGB_affine_weights_offset,synt_weights_offset.b64,tRGB_weights_offset.b64-use-dom-mod-out_in_additive,affine_out_in_5_1_additive',\n",
    "}\n",
    "\n",
    "i2i_experiments = {\n",
    "    'Dog': afhqdog,\n",
    "    'Cat': afhqcat,\n",
    "    'Wild': afhqwild,\n",
    "}\n",
    "\n",
    "chkpt_idxs = {\n",
    "    'Dog': 241,\n",
    "    'Cat': 241,\n",
    "    'Wild': 241,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6cc805",
   "metadata": {},
   "source": [
    "Then, you need to perform $Z$ space inversion for images from the validation part of the AFHQ dataset (for all three domains). Suppose you have this dataset on the following path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b7cb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:04:38.028060Z",
     "start_time": "2023-03-15T18:04:37.982687Z"
    }
   },
   "outputs": [],
   "source": [
    "afhq_path = './afhq'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339fc9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T20:26:31.571199Z",
     "start_time": "2023-01-24T20:26:31.465627Z"
    }
   },
   "source": [
    "For each parameterization and for each domain, perform image inversion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bdb296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:04:39.751074Z",
     "start_time": "2023-03-15T18:04:38.029554Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset_name, dataset_exps in i2i_experiments.items():\n",
    "    chkpt_idx = chkpt_idxs[dataset_name]\n",
    "    \n",
    "    for pname, exp_suffix in dataset_exps.items():\n",
    "        for image_path in glob.glob(os.path.join(afhq_path, f'val/{dataset_name.lower()}/*.jpg')):\n",
    "            outdir_path = os.path.join(\n",
    "                f'./inverted_samples/afhq/val/{dataset_name.lower()}/{pname}', \n",
    "                os.path.splitext(os.path.basename(image_path))[0]\n",
    "            )\n",
    "            if os.path.exists(os.path.join(outdir_path, 'projected_z.npz')):\n",
    "                continue\n",
    "            \n",
    "            network_path = os.path.join(\n",
    "                base_exp_path, exp_suffix, 'network-snapshot-{0:06d}.pkl'.format(chkpt_idx)\n",
    "            )\n",
    "\n",
    "            ! python ./../projector.py --target \"$image_path\" --outdir \"$outdir_path\" --network \"$network_path\" \\\n",
    "                --space 'z' --truncation-psi 0.7  --num-steps 1000 \\\n",
    "                --gpu 0 --save-image --save-video --save-all-steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8d124",
   "metadata": {},
   "source": [
    "Load all checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7d1da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:05:03.670215Z",
     "start_time": "2023-03-15T18:04:39.754242Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_models = defaultdict(defaultdict)\n",
    "\n",
    "for dataset_name, dataset_exps in i2i_experiments.items():\n",
    "    chkpt_idx = chkpt_idxs[dataset_name]\n",
    "    \n",
    "    for parameterization_name, exp_suffix in dataset_exps.items():\n",
    "        exp_path = os.path.join(base_exp_path, exp_suffix)\n",
    "        _, G_ema, _, _, _, _ = nb_utils.load_checkpoint(\n",
    "            exp_path=exp_path, chkpt_idx=chkpt_idx, device=torch.device('cpu')\n",
    "        )\n",
    "\n",
    "        all_models[parameterization_name][dataset_name] = G_ema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0ee67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T08:55:21.402481Z",
     "start_time": "2023-03-24T08:55:21.395646Z"
    }
   },
   "source": [
    "### Figure 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526270b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:42:22.265555Z",
     "start_time": "2023-03-15T18:42:22.151874Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device):\n",
    "    grid_size = np.array([len(images_paths), len([_ for _ in all_models.values() if target_dataset_name in _]) + 1])\n",
    "    fig, axes = plt.subplots(*grid_size, figsize=grid_size[::-1] * 3)\n",
    "    axes = axes.reshape(grid_size)\n",
    "\n",
    "    nb_utils.prepare_axes(axes)\n",
    "    for idx, image_path in enumerate(images_paths):\n",
    "        axes[idx, 0].imshow(plt.imread(image_path))\n",
    "\n",
    "        for jdx, pname in enumerate(['Full', 'Affine+', 'AffineLight+']):\n",
    "            result_path = os.path.join(\n",
    "                    f'./inverted_samples/afhq/val/{source_dataset_name.lower()}/{pname}', \n",
    "                    os.path.splitext(os.path.basename(image_path))[0]\n",
    "                )\n",
    "            result = np.load(os.path.join(result_path, '0_projected_z.npz'), allow_pickle=True)\n",
    "\n",
    "            images, _ = nb_utils.generate_images(\n",
    "                all_models[pname][target_dataset_name], grid_size=1, device=device, truncation_psi=0.8,\n",
    "                target_zs=torch.tensor(result['z']).to(device)\n",
    "            )\n",
    "            axes[idx, jdx + 1].imshow(images[0])\n",
    "\n",
    "            if idx == 0:\n",
    "                axes[idx, jdx + 1].set_title(pname, fontdict=dict(fontsize=16, weight='bold'))\n",
    "\n",
    "    axes[0, 0].set_title('Input', fontdict=dict(fontsize=16, weight='bold'))\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3016e4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Cat2Dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283f41c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:42:28.734712Z",
     "start_time": "2023-03-15T18:42:25.057412Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Cat', 'Dog', [\n",
    "    os.path.join(afhq_path, 'val/cat/flickr_cat_000816.jpg'),\n",
    "    os.path.join(afhq_path, 'val/cat/flickr_cat_000320.jpg'),\n",
    "    os.path.join(afhq_path, 'val/cat/pixabay_cat_000081.jpg')\n",
    "]\n",
    "fig = make_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)\n",
    "fig.savefig('./../images/I2I_3_Cat2Dog.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47365c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Cat2Wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b506c9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:42:32.253521Z",
     "start_time": "2023-03-15T18:42:28.738604Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Cat', 'Wild', [\n",
    "    os.path.join(afhq_path, 'val/cat/pixabay_cat_000615.jpg'),\n",
    "    os.path.join(afhq_path, 'val/cat/flickr_cat_000265.jpg'),\n",
    "    os.path.join(afhq_path, 'val/cat/pixabay_cat_000668.jpg')\n",
    "]\n",
    "fig = make_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)\n",
    "fig.savefig('./../images/I2I_3_Cat2Wild.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768d791",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Dog2Wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b911b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:42:35.602957Z",
     "start_time": "2023-03-15T18:42:32.256170Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Dog', 'Wild', [\n",
    "    os.path.join(afhq_path, 'val/dog/pixabay_dog_000307.jpg'),\n",
    "    os.path.join(afhq_path, 'val/dog/pixabay_dog_000818.jpg'),\n",
    "    os.path.join(afhq_path, 'val/dog/flickr_dog_000619.jpg')\n",
    "]\n",
    "fig = make_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)\n",
    "fig.savefig('./../images/I2I_3_Dog2Wild.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef23670",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Dog2Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247964c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:42:39.242959Z",
     "start_time": "2023-03-15T18:42:35.605535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Dog', 'Cat', [\n",
    "    os.path.join(afhq_path, 'val/dog/flickr_dog_000176.jpg'),\n",
    "    os.path.join(afhq_path, 'val/dog/flickr_dog_000569.jpg'),\n",
    "    os.path.join(afhq_path, 'val/dog/pixabay_dog_000494.jpg')\n",
    "]\n",
    "fig = make_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)\n",
    "fig.savefig('./../images/I2I_3_Dog2Cat.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc38d73",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Wild2Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3110b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:42:43.021115Z",
     "start_time": "2023-03-15T18:42:39.248112Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Wild', 'Cat', [\n",
    "    os.path.join(afhq_path, 'val/wild/flickr_wild_001627.jpg'),\n",
    "    os.path.join(afhq_path, 'val/wild/flickr_wild_003586.jpg'),\n",
    "    os.path.join(afhq_path, 'val/wild/pixabay_wild_000267.jpg')\n",
    "]\n",
    "fig = make_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)\n",
    "fig.savefig('./../images/I2I_3_Wild2Cat.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6719a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Wild2Dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac8b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:42:46.649758Z",
     "start_time": "2023-03-15T18:42:43.023180Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Wild', 'Dog', [\n",
    "    os.path.join(afhq_path, 'val/wild/flickr_wild_003060.jpg'),\n",
    "    os.path.join(afhq_path, 'val/wild/flickr_wild_002036.jpg'),\n",
    "    os.path.join(afhq_path, 'val/wild/flickr_wild_000063.jpg')\n",
    "]\n",
    "fig = make_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)\n",
    "fig.savefig('./../images/I2I_3_Wild2Dog.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c664838",
   "metadata": {},
   "source": [
    "### Figure 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622431e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ref_i2i_inference(\n",
    "    source_dataset_name, target_dataset_name, images_paths, all_models, device, *,\n",
    "    truncation_psi=0.8, reference_slice=4\n",
    "):\n",
    "    grid_size = np.array([len(images_paths), len([_ for _ in all_models.values() if target_dataset_name in _]) + 2])\n",
    "    fig, axes = plt.subplots(*grid_size, figsize=grid_size[::-1] * 2.5)\n",
    "    axes = axes.reshape(grid_size)\n",
    "\n",
    "    nb_utils.prepare_axes(axes)\n",
    "    for idx, (image_path, ref_image_path) in enumerate(images_paths):\n",
    "        axes[idx, 0].imshow(plt.imread(image_path))\n",
    "        axes[idx, 1].imshow(plt.imread(ref_image_path))\n",
    "\n",
    "        for jdx, pname in enumerate(['Full', 'Affine+', 'AffineLight+']):\n",
    "            result_path = os.path.join(\n",
    "                f'./inverted_samples/afhq/val/{source_dataset_name.lower()}/{pname}', \n",
    "                os.path.splitext(os.path.basename(image_path))[0]\n",
    "            )\n",
    "            result = np.load(os.path.join(result_path, '0_projected_z.npz'), allow_pickle=True)\n",
    "\n",
    "            ref_result_path = os.path.join(\n",
    "                f'./inverted_samples/afhq/val/{target_dataset_name.lower()}/{pname}', \n",
    "                os.path.splitext(os.path.basename(ref_image_path))[0]\n",
    "            )\n",
    "            ref_result = np.load(os.path.join(ref_result_path, '0_projected_z.npz'), allow_pickle=True)\n",
    "\n",
    "            c = torch.empty([result['z'].shape[0], 0])\n",
    "            all_models[pname][target_dataset_name].to(device).eval()\n",
    "            ws = all_models[pname][target_dataset_name].mapping(\n",
    "                torch.tensor(result['z']).to(device), c.to(device), truncation_psi=0.8\n",
    "            )\n",
    "            ref_ws = all_models[pname][target_dataset_name].mapping(\n",
    "                torch.tensor(ref_result['z']).to(device), c.to(device), truncation_psi=0.8\n",
    "            )\n",
    "            ws[:, reference_slice:] = ref_ws[:, reference_slice:]\n",
    "\n",
    "            images, _ = nb_utils.generate_images(\n",
    "                all_models[pname][target_dataset_name], grid_size=1, device=device,\n",
    "                target_ws=ws\n",
    "            )\n",
    "            axes[idx, jdx + 2].imshow(images[0])\n",
    "\n",
    "            if idx == 0:\n",
    "                axes[idx, jdx + 2].set_title(pname, fontdict=dict(fontsize=14, weight='bold'))\n",
    "\n",
    "    axes[0, 0].set_title('Source', fontdict=dict(fontsize=14, weight='bold'))\n",
    "    axes[0, 1].set_title('Reference', fontdict=dict(fontsize=14, weight='bold'))\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993681e1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Cat2Dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513496bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:42:50.162064Z",
     "start_time": "2023-03-15T18:42:46.654859Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Cat', 'Dog', [\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/cat/flickr_cat_000123.jpg'),\n",
    "        os.path.join(afhq_path, 'val/dog/pixabay_dog_002544.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/cat/pixabay_cat_002582.jpg'),\n",
    "        os.path.join(afhq_path, 'val/dog/flickr_dog_000452.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/cat/pixabay_cat_003562.jpg'),\n",
    "        os.path.join(afhq_path, 'val/dog/pixabay_dog_000607.jpg'),\n",
    "    )\n",
    "]\n",
    "fig = make_ref_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b6786",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Cat2Wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b0f8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:42:53.637541Z",
     "start_time": "2023-03-15T18:42:50.165006Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Cat', 'Wild', [\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/cat/pixabay_cat_000343.jpg'),\n",
    "        os.path.join(afhq_path, 'val/wild/pixabay_wild_001082.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/cat/pixabay_cat_000147.jpg'),\n",
    "        os.path.join(afhq_path, 'val/wild/flickr_wild_001137.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/cat/pixabay_cat_004765.jpg'),\n",
    "        os.path.join(afhq_path, 'val/wild/flickr_wild_003947.jpg'),\n",
    "    )\n",
    "]\n",
    "fig = make_ref_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54adce30",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Dog2Wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e659b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:42:57.227366Z",
     "start_time": "2023-03-15T18:42:53.640278Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Dog', 'Wild', [\n",
    "    (\n",
    "       os.path.join(afhq_path, 'val/dog/pixabay_dog_003449.jpg'),\n",
    "       os.path.join(afhq_path, 'val/wild/flickr_wild_001625.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/dog/pixabay_dog_001651.jpg'),\n",
    "        os.path.join(afhq_path, 'val/wild/pixabay_wild_000265.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/dog/flickr_dog_001100.jpg'),\n",
    "        os.path.join(afhq_path, 'val/wild/pixabay_wild_000536.jpg'),\n",
    "    )\n",
    "]\n",
    "fig = make_ref_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4da221",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Dog2Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd73c3bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:43:00.656194Z",
     "start_time": "2023-03-15T18:42:57.229526Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Dog', 'Cat', [\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/dog/pixabay_dog_000504.jpg'),\n",
    "        os.path.join(afhq_path, 'val/cat/flickr_cat_000446.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/dog/pixabay_dog_002307.jpg'),\n",
    "        os.path.join(afhq_path, 'val/cat/pixabay_cat_004217.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/dog/pixabay_dog_002423.jpg'),\n",
    "        os.path.join(afhq_path, 'val/cat/flickr_cat_000585.jpg'),\n",
    "    )\n",
    "]\n",
    "fig = make_ref_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8970c52",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Wild2Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413330b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:43:04.138153Z",
     "start_time": "2023-03-15T18:43:00.658257Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Wild', 'Cat', [\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/wild/flickr_wild_001230.jpg'),\n",
    "        os.path.join(afhq_path, 'val/cat/pixabay_cat_001632.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/wild/flickr_wild_002336.jpg'),\n",
    "        os.path.join(afhq_path, 'val/cat/pixabay_cat_001029.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/wild/flickr_wild_000418.jpg'),\n",
    "        os.path.join(afhq_path, 'val/cat/pixabay_cat_002559.jpg'),\n",
    "    )\n",
    "]\n",
    "fig = make_ref_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f683a4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Wild2Dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724abed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T18:43:07.691011Z",
     "start_time": "2023-03-15T18:43:04.141774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_dataset_name, target_dataset_name, images_paths = 'Wild', 'Dog', [\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/wild/flickr_wild_003854.jpg'),\n",
    "        os.path.join(afhq_path, 'val/dog/pixabay_dog_002700.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/wild/flickr_wild_003169.jpg'),\n",
    "        os.path.join(afhq_path, 'val/dog/pixabay_dog_002680.jpg'),\n",
    "    ),\n",
    "    (\n",
    "        os.path.join(afhq_path, 'val/wild/flickr_wild_002867.jpg'),\n",
    "        os.path.join(afhq_path, 'val/dog/pixabay_dog_002597.jpg'),\n",
    "    )\n",
    "]\n",
    "fig = make_ref_i2i_inference(source_dataset_name, target_dataset_name, images_paths, all_models, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
