{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61329883-621c-40db-b5c9-590294dc5b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1970f10-0502-464e-95a1-7669ea5605eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "from torchvision.transforms import Resize\n",
    "from itertools import combinations, product\n",
    "from pathlib import Path\n",
    "\n",
    "from core.uda_models import uda_models\n",
    "from core.utils.common import mixing_noise\n",
    "from core.utils.example_utils import to_im, Inferencer, vstack_with_lines, hstack_with_lines, insert_image\n",
    "from core.utils.reading_weights import read_weights\n",
    "\n",
    "from draw_util import IdentityEditor, StyleEditor, morph_g_ema, weights, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59e30c-de42-452c-85e5-fea35a61d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# styleflow weights\n",
    "\n",
    "!wget https://nxt.2a2i.org/index.php/s/yxdCXxSWJgAKXkP/download/styleflow_data.zip -O editing/styleflow/styleflow_data.zip\n",
    "!unzip editing/styleflow/styleflow_data.zip -d editing/styleflow\n",
    "!rm -rf editing/styleflow/styleflow_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961ff0e-7e6d-4d5d-92f7-94e9d2bc2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "\n",
    "## Setup InterFaceGAN\n",
    "\n",
    "interfacegan_ckpts = {\n",
    "    'age': torch.load('editing/interfacegan_directions/age.pt').to(device),\n",
    "    'rotation': torch.load('editing/interfacegan_directions/rotation.pt').to(device),\n",
    "    'smile': torch.load('editing/interfacegan_directions/smile.pt').to(device),\n",
    "    'gender': torch.load('editing/interfacegan_directions/gender.pt').to(device)\n",
    "}\n",
    "\n",
    "def edit_ifgan(styles, attr, power):\n",
    "    return [styles[0] + power * interfacegan_ckpts[attr]]\n",
    "\n",
    "\n",
    "## Setup StyleSpace editing\n",
    "\n",
    "style_space_config = {\n",
    "    'black hair' :      [(12, 479)],\n",
    "    'blond hair ':      [(12, 479), (12, 266)],\n",
    "    'grey hair ' :      [(11, 286)],\n",
    "    'wavy hair'  :      [(6, 500), (8, 128), (5, 92), (6, 394), (6, 323)],\n",
    "    'bangs'      :      [(3, 259), (6, 285), (5, 414), (6, 128), (9, 295), (6, 322), (6, 487), (6, 504)],\n",
    "    'receding hairline':[(5, 414), (6, 322), (6, 497), (6, 504)],\n",
    "    'smiling'    :      [(6, 501)],\n",
    "    'lipstick'   :      [(15, 45)],\n",
    "    'sideburns'  :      [(12, 237)],\n",
    "    'goatee'     :      [(9, 421)],\n",
    "    'earrings'   :      [(8, 81)],\n",
    "    'glasses'    :      [(3, 288), (2, 175), (3, 120), (2, 97)],\n",
    "    'wear suit'  :      [(9, 441), (8, 292), (11, 358), (6, 223)],\n",
    "    'gender'     :      [(9, 6)]\n",
    "}\n",
    "\n",
    "\n",
    "def edit_stylespace(styles, loc, power):\n",
    "    st_idx, ch_idx = loc\n",
    "    styles_tmp = [s.clone() for s in styles]\n",
    "    styles_tmp[st_idx][:, ch_idx] += power\n",
    "    return styles_tmp\n",
    "\n",
    "\n",
    "## Setup StyleFlow Editing\n",
    "\n",
    "\n",
    "from editing import StyleFlowEditor\n",
    "\n",
    "# download weights from https://github.com/RameenAbdal/StyleFlow\n",
    "# StyleFlow/flow_weight/modellarge10k.pt -> editing/modellarge10k.pt\n",
    "# StyleFlow/data -> editing/styleflow_data\n",
    "\n",
    "styleflow_model_path = 'editing/styleflow/styleflow_data/flow_weight/modellarge10k.pt'\n",
    "styleflow_data_path = 'editing/styleflow/styleflow_data/data'\n",
    "\n",
    "editor = StyleFlowEditor(styleflow_data_path, styleflow_model_path, device)\n",
    "\n",
    "styleflow_attr_order = ['gender', 'glasses', 'yaw', 'pitch', 'baldness', 'beard', 'age', 'expression']\n",
    "styleflow_attr_to_idx = dict(zip(styleflow_attr_order, range(len(styleflow_attr_order))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c3e17-7a51-47a0-a260-f3ac554d81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\"editing model\", \"special context for editing\", \"attribute name\", \"editing power\")\n",
    "# Only for style_space editing model context is used see `style_space_config` for details\n",
    "# For attribute name pick one pair of style space positions\n",
    "\n",
    "\n",
    "columns = [\n",
    "    ('no_editing', None, None, None),\n",
    "    ('style_space', (12, 479), 'black hair', 14), # + чернее\n",
    "    ('style_space', (6, 501), 'smile', -6), # - в улыбку\n",
    "    ('style_space', (15, 45), 'lipstick', -10), # - в покраску красным\n",
    "    # ('if_gan', None, 'gender', 7), # + в улыбку\n",
    "    ('if_gan', None, 'rotation', -5), # - направо\n",
    "    ('if_gan', None, 'age', -3), # + выше\n",
    "    ('styleflow', None, 'gender', 0.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7ffa1-217e-4ec0-a969-0e2e1415e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    ('original', None),\n",
    "    ('sketch', 'td'),\n",
    "    ('pixar', 'td'),\n",
    "    ('anime', 'td'),\n",
    "    ('joker', 'td'),\n",
    "    ('zombie', 'td'),\n",
    "    ('anastasia', 'im2im'),\n",
    "    ('mermaid', 'im2im')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa184a-1aff-4eda-9d66-8037b80f4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup latents (from styleflow) and inference parameters\n",
    "\n",
    "truncation = 0.8\n",
    "offset_power = 0.7\n",
    "linear_size = 128\n",
    "\n",
    "\n",
    "editor._allocate_entity(6)\n",
    "w_original = [torch.from_numpy(editor.w_current_np[:, 0, :]).to(device).float()]\n",
    "w_original = [w_original[0].unsqueeze(1).repeat(1, 18, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833e879-7edd-4141-bc11-f58eba3f02d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031fcbd0-45fa-4591-a70b-8e239a5342cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = []\n",
    "\n",
    "for row_domain, da_type in rows:\n",
    "    \n",
    "    row_image = []\n",
    "    for edit_type, ctx, attr_name, power in columns:        \n",
    "        if row_domain == 'original':\n",
    "            ckpt = read_weights(weights[rows[1][0]])\n",
    "        else:\n",
    "            ckpt = read_weights(weights[row_domain])\n",
    "        \n",
    "        model = Inferencer(ckpt, device)\n",
    "        \n",
    "        if 'style_latents' in ckpt and row_domain != 'original':\n",
    "            w_current = w_original[0].clone()\n",
    "            w_current[:, 7:, :] = ckpt['style_latents'].to(device)\n",
    "            w = [w_current]\n",
    "        else:\n",
    "            w = w_original\n",
    "        \n",
    "        if row_domain == 'original':\n",
    "            if edit_type == 'style_space':\n",
    "                s_code = model.sg2_source.get_s_code(w, input_is_latent=True, truncation=truncation)\n",
    "                s_code_edited_attr = edit_stylespace(s_code, ctx, power)\n",
    "                trg_im, _ = model.sg2_source(s_code_edited_attr, is_s_code=True)\n",
    "\n",
    "            elif edit_type == 'if_gan':\n",
    "                w_edited = edit_ifgan(w, attr_name, power)\n",
    "                trg_im, _ = model(\n",
    "                    w_edited, input_is_latent=True, \n",
    "                    truncation=truncation, \n",
    "                    offset_power=offset_power\n",
    "                )\n",
    "            elif edit_type == 'styleflow':\n",
    "                _, w_edited = editor.get_edited_pair(styleflow_attr_to_idx[attr_name], power)\n",
    "                trg_im, _ = model([w_edited], input_is_latent=True, truncation=truncation, offset_power=offset_power)\n",
    "            else:\n",
    "                trg_im, _ = model(\n",
    "                    w, input_is_latent=True, \n",
    "                    truncation=truncation, \n",
    "                    offset_power=offset_power\n",
    "                )\n",
    "        else:\n",
    "            if edit_type == 'style_space':\n",
    "                s_code = model.sg2_source.get_s_code(w, input_is_latent=True, truncation=truncation)\n",
    "                domain_editor = StyleEditor(ckpt, device)\n",
    "\n",
    "                s_code_da_edited = domain_editor(s_code, offset_power)\n",
    "                s_code_edited_attr = edit_stylespace(s_code_da_edited, ctx, power)\n",
    "                trg_im, _ = model.sg2_source(s_code_edited_attr, is_s_code=True)\n",
    "\n",
    "            elif edit_type == 'if_gan':\n",
    "                w_edited = edit_ifgan(w, attr_name, power)\n",
    "                _, trg_im = model(\n",
    "                    w_edited, input_is_latent=True, \n",
    "                    truncation=truncation, \n",
    "                    offset_power=offset_power\n",
    "                )\n",
    "            elif edit_type == 'styleflow':\n",
    "                _, w_edited = editor.get_edited_pair(styleflow_attr_to_idx[attr_name], power)\n",
    "                _, trg_im = model([w_edited], input_is_latent=True, truncation=truncation, offset_power=offset_power)\n",
    "            else:\n",
    "                _, trg_im = model(\n",
    "                    w, input_is_latent=True, \n",
    "                    truncation=truncation, \n",
    "                    offset_power=offset_power\n",
    "                )\n",
    "        \n",
    "        row_image.append(Resize(linear_size)(to_im(trg_im, nrow=2)))\n",
    "    \n",
    "    if da_type == 'im2im':\n",
    "        pil_image = PIL.Image.open(f'image_domains/{row_domain}.png') \n",
    "        pil_image = pil_image.resize((linear_size - 16, linear_size - 16))\n",
    "        additional_image = insert_image(np.array(pil_image), 128)\n",
    "        \n",
    "    else:\n",
    "        additional_image = np.ones((linear_size, linear_size, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    image.append([additional_image] + row_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11b5d1-3803-449a-82e9-c3a1153c0179",
   "metadata": {},
   "source": [
    "## Draw Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8e9e8-9614-4016-bf65-ee2548c49356",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_width = 10\n",
    "skip_vertical = 15\n",
    "\n",
    "row_h, row_w = hstack_with_lines(image[0], skip_width).shape[:2]\n",
    "\n",
    "final_image = [np.ones((row_h, row_w, 3), dtype=np.uint8) * 255]\n",
    "\n",
    "for row_stack in image:\n",
    "    final_image.append(hstack_with_lines(row_stack, skip_width))\n",
    "    \n",
    "final_image = vstack_with_lines(final_image, skip_vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd120b74-1776-46cb-81b4-79e58eeb8973",
   "metadata": {},
   "outputs": [],
   "source": [
    "abb_to_cap = {\n",
    "    'original': 'Original', 'pixar': 'Pixar', 'sketch': 'Sketch', 'joker': 'The Joker', \n",
    "    'zombie': 'Zombie', 'anime': 'Anime', 'botero': 'Botero', 'pop_art': 'Pop Art',\n",
    "    'black hair': 'Black\\nHair', 'smile': 'Smile', 'lipstick': 'Lipstick', 'rotation':  'Rotation', \n",
    "    'age': 'Age', 'gender': 'Gender',\n",
    "    None: ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0f46c-9ac6-4478-b316-f9e764ccf4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "num = 3\n",
    "\n",
    "plt.figure(figsize=(num * len(columns), num * len(rows) // 2))\n",
    "plt.imshow(final_image)\n",
    "plt.axis('off')\n",
    "\n",
    "for i, (_, _, attr, _) in enumerate(columns):\n",
    "    plt.text(linear_size // 2 + (linear_size + skip_width) * (i + 1), linear_size // 2, \n",
    "             abb_to_cap[attr], fontsize=16, weight='bold',\n",
    "             horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "\n",
    "for i, (domain, da_type) in enumerate(rows):\n",
    "    if da_type == 'im2im':\n",
    "        continue\n",
    "    plt.text(linear_size // 2, linear_size * 3 // 2 + skip_vertical + (linear_size + skip_vertical) * i, \n",
    "             abb_to_cap[domain], fontsize=16, weight='bold', \n",
    "             horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67861b-8e73-4876-ac84-5e08658402f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9004e54a-58b1-47da-940f-b5604ee97f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
